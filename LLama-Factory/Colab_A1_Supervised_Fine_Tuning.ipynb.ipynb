{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oHFCsV0z-Jw"
      },
      "source": [
        "# Finetune Llama-3 with LLaMA Factory\n",
        "\n",
        "Please use a **free** Tesla T4 Colab GPU to run this!\n",
        "\n",
        "Project homepage: https://github.com/hiyouga/LLaMA-Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "89e6d6ef166d49c3a4b017d041b9833b",
            "e6554619029345a3ac25b092ce68283f",
            "cef78e80bbe4449083ceefcd03853423",
            "4af063573c1f4dad945ef0954b1280e5",
            "eaa684eeb1914f26aacbf5599200c75d",
            "c12069af9f454083bdacc874a62534fd",
            "89f4f7137e8242ba85c59eff29ea2900",
            "9a6942125b024a8ea137f14e204e876b",
            "802f7639a1ec4f7d8d19930ae0d50559",
            "53cd9ae24080479a85a6135ee345259f",
            "20605af761d94a97b102ef44138b0028",
            "19ba8358feed4049b49a4a530605c52b",
            "fcf5109c259441ceb0a92e15236ea3d2",
            "b86b008d321345b2b8bdaecd9c4e2fc4",
            "c1d49c792fd045b3b4e8747a8753b62a",
            "1473b696888a444983d745882f59acd6",
            "3c1c6fcc397a4d4dad0c699832ca1b22",
            "b0048e5323d74f08afe8b58c976f98ae",
            "540b108657934a6ea42932e0a424c870",
            "0e6d417614b545229b93dee08e157f82",
            "93184fb2e5d144f6b1891f7c03bb9a30",
            "3ae4731ab2fe43e9aed51352b7706551",
            "0290a2e40e7840e9ac8d8889fd386f73",
            "a33d68d9a3f941448c96feb485b35833",
            "67e8a2f9635a4011ac295a2b6220b8b4",
            "9dbd52d941c040509dfb90b87aaf1825",
            "339a85f8e7dd47b4b61dc1bdde400817",
            "81833313b36c4ea4b8545059f1b85d00",
            "c6b8d0eddfb44058a763c5893675bff7"
          ]
        },
        "id": "uQo4aPfQryil",
        "outputId": "70f7cd23-f7e6-40e6-ccb2-93d995cbc9ea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89e6d6ef166d49c3a4b017d041b9833b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Prompt for Hugging Face login\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr7rB3szzhtx"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giM74oK1rRIH",
        "outputId": "6feaf164-8730-440c-d88b-321f35c4edef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 315, done.\u001b[K\n",
            "remote: Counting objects: 100% (315/315), done.\u001b[K\n",
            "remote: Compressing objects: 100% (245/245), done.\u001b[K\n",
            "remote: Total 315 (delta 80), reused 160 (delta 57), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (315/315), 8.94 MiB | 16.61 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdocker\u001b[0m/      LICENSE      pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mevaluation\u001b[0m/  Makefile     README.md       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mexamples\u001b[0m/    MANIFEST.in  README_zh.md    setup.py\n",
            "Requirement already satisfied: torch==2.3.1 in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: torchvision==0.18.1 in /usr/local/lib/python3.10/dist-packages (0.18.1)\n",
            "Requirement already satisfied: torchaudio==2.3.1 in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.1) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.1) (9.4.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.6.68)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.1) (1.3.0)\n",
            "\u001b[33mWARNING: Skipping jax as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mObtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<=4.45.0,>=4.41.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (4.44.2)\n",
            "Requirement already satisfied: datasets<=2.21.0,>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.21.0)\n",
            "Requirement already satisfied: accelerate<=0.34.2,>=0.30.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.34.2)\n",
            "Requirement already satisfied: peft<=0.12.0,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.12.0)\n",
            "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.9.6)\n",
            "Requirement already satisfied: gradio>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (4.44.0)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (1.13.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.8.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.1.99)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.7.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (3.20.3)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.30.6)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.9.1)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.115.0)\n",
            "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.1.3)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (3.7.1)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (1.26.4)\n",
            "Requirement already satisfied: liger-kernel in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.3.0)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.3.1)\n",
            "Requirement already satisfied: bitsandbytes>=0.39.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.43.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.34.2,>=0.30.1->llamafactory==0.9.1.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.34.2,>=0.30.1->llamafactory==0.9.1.dev0) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.34.2,>=0.30.1->llamafactory==0.9.1.dev0) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.16.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.10.5)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.7.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.27.2)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.10.7)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (9.4.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.0.9)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.6.5)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (2.0.7)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio>=4.0.0->llamafactory==0.9.1.dev0) (12.0)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->llamafactory==0.9.1.dev0) (0.38.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.1.dev0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.1.dev0) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.1.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.1.dev0) (2.23.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (3.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->llamafactory==0.9.1.dev0) (12.6.68)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.45.0,>=4.41.2->llamafactory==0.9.1.dev0) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.45.0,>=4.41.2->llamafactory==0.9.1.dev0) (0.19.1)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0) (0.8.10)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.1.dev0) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.1.dev0) (0.14.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.1.dev0) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.1.dev0) (2.4.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.9.1.dev0) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.9.1.dev0) (1.2.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.9.1.dev0) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.9.1.dev0) (1.0.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.3.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (13.8.1)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0) (1.7.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->llamafactory==0.9.1.dev0) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.1.2)\n",
            "Building wheels for collected packages: llamafactory\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.1.dev0-0.editable-py3-none-any.whl size=22355 sha256=0f9ce27ac2551db55493c69b1dcad405f8ec9e2265f84ae3474cbe258a72881e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-piq2lmr6/wheels/de/aa/c5/27b5682c5592b7c0eecc3e208f176dedf6b11a61cf2a910b85\n",
            "Successfully built llamafactory\n",
            "Installing collected packages: llamafactory\n",
            "  Attempting uninstall: llamafactory\n",
            "    Found existing installation: llamafactory 0.9.1.dev0\n",
            "    Uninstalling llamafactory-0.9.1.dev0:\n",
            "      Successfully uninstalled llamafactory-0.9.1.dev0\n",
            "Successfully installed llamafactory-0.9.1.dev0\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
        "!pip uninstall -y jax\n",
        "!pip install -e .[torch,bitsandbytes,liger-kernel]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9RXn_YQnn9f"
      },
      "source": [
        "### Check GPU environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZkN-ktlsnrdU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeYs5Lz-QJYk"
      },
      "source": [
        "## Update Identity Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap_fvMBsQHJc",
        "outputId": "74c36531-799d-46c7-a9e1-b2054dd73d14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "# Update model name and author to reflect the new model being used\n",
        "NAME = \"Gemma-2B\"\n",
        "AUTHOR = \"LLaMA Factory\"\n",
        "\n",
        "# Open and read the dataset file\n",
        "with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  dataset = json.load(f)\n",
        "\n",
        "# Replace placeholders with the updated name and author\n",
        "for sample in dataset:\n",
        "  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
        "\n",
        "# Write the updated dataset back to the file\n",
        "with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(dataset, f, indent=2, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QiXcvdzzW3Y"
      },
      "source": [
        "## Fine-tune model via LLaMA Board"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLsdS6V5yUMy",
        "outputId": "c0b42bc5-79ee-4afa-b1d6-04d88e3d00de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2024-09-18 04:01:40.437128: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-18 04:01:40.476626: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-18 04:01:40.488704: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-18 04:01:40.516941: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-18 04:01:42.528451: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "Running on public URL: https://5b38c01651bc148977.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "2024-09-18 04:04:51.899043: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-18 04:04:51.923010: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-18 04:04:51.930148: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-18 04:04:51.947008: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-18 04:04:53.315903: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "09/18/2024 04:05:00 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/google/gemma-2b/resolve/main/config.json\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1752, in _get_metadata_or_catch_error\n",
            "    metadata = get_hf_file_metadata(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1674, in get_hf_file_metadata\n",
            "    r = _request_wrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 376, in _request_wrapper\n",
            "    response = _request_wrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 400, in _request_wrapper\n",
            "    hf_raise_for_status(response)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 367, in hf_raise_for_status\n",
            "    raise HfHubHTTPError(message, response=response) from e\n",
            "huggingface_hub.utils._errors.HfHubHTTPError:  (Request ID: Root=1-66ea516d-3fd237e14fc6071318af2712;0e8bdd61-b434-4cf0-894d-143bdb2a869b)\n",
            "\n",
            "403 Forbidden: Please enable access to public gated repositories in your fine-grained token settings to view this repository..\n",
            "Cannot access content at: https://huggingface.co/google/gemma-2b/resolve/main/config.json.\n",
            "If you are trying to create or update content, make sure you have a token with the `write` role.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 402, in cached_file\n",
            "    resolved_file = hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1240, in hf_hub_download\n",
            "    return _hf_hub_download_to_cache_dir(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1347, in _hf_hub_download_to_cache_dir\n",
            "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1858, in _raise_on_head_call_error\n",
            "    raise LocalEntryNotFoundError(\n",
            "huggingface_hub.utils._errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 111, in main\n",
            "    run_exp()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
            "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 44, in run_sft\n",
            "    tokenizer_module = load_tokenizer(model_args)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/model/loader.py\", line 69, in load_tokenizer\n",
            "    config = load_config(model_args)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/model/loader.py\", line 122, in load_config\n",
            "    return AutoConfig.from_pretrained(model_args.model_name_or_path, **init_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 976, in from_pretrained\n",
            "    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 632, in get_config_dict\n",
            "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 689, in _get_config_dict\n",
            "    resolved_config_file = cached_file(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 445, in cached_file\n",
            "    raise EnvironmentError(\n",
            "OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like google/gemma-2b is not the path to a directory containing a file named config.json.\n",
            "Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n",
            "2024-09-18 04:10:18.620401: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-18 04:10:18.644124: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-18 04:10:18.651243: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-18 04:10:18.668655: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-18 04:10:20.000839: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "09/18/2024 04:10:26 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
            "config.json: 100% 627/627 [00:00<00:00, 3.36MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-09-18 04:10:26,755 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-09-18 04:10:26,760 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 33.6k/33.6k [00:00<00:00, 41.7MB/s]\n",
            "tokenizer.model: 100% 4.24M/4.24M [00:00<00:00, 150MB/s]\n",
            "tokenizer.json: 100% 17.5M/17.5M [00:00<00:00, 188MB/s]\n",
            "special_tokens_map.json: 100% 636/636 [00:00<00:00, 4.35MB/s]\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-18 04:10:29,202 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-18 04:10:29,202 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-18 04:10:29,202 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-18 04:10:29,203 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-18 04:10:29,203 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:733] 2024-09-18 04:10:31,218 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-09-18 04:10:31,219 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-18 04:10:31,478 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-18 04:10:31,478 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-18 04:10:31,478 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-18 04:10:31,478 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-09-18 04:10:31,478 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/tokenizer_config.json\n",
            "09/18/2024 04:10:32 - INFO - llamafactory.data.loader - Loading dataset llamafactory/lima...\n",
            "Downloading readme: 100% 450/450 [00:00<00:00, 1.80kB/s]\n",
            "Downloading data: 100% 3.07M/3.07M [00:01<00:00, 2.64MB/s]\n",
            "Generating train split: 1029 examples [00:00, 7831.00 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100% 1029/1029 [00:01<00:00, 942.47 examples/s] \n",
            "Running tokenizer on dataset (num_proc=16): 100% 1029/1029 [00:34<00:00, 29.65 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[20279, 235292, 2551, 9494, 5999, 3124, 235336, 3339, 8069, 590, 2714, 1497, 6075, 23813, 591, 178722, 2819, 573, 9494, 1297, 846, 108, 51190, 235292, 651, 2872, 603, 11873, 7209, 578, 974, 1412, 1987, 1280, 3185, 674, 573, 9494, 780, 1297, 13569, 576, 44259, 235269, 901, 1170, 9173, 492, 5999, 591, 14186, 105065, 5999, 235275, 578, 953, 235290, 1202, 18113, 96057, 13265, 5999, 235265, 22174, 235269, 685, 9523, 11648, 235290, 129394, 3537, 791, 11969, 235269, 47492, 6325, 603, 1508, 2845, 235269, 685, 573, 11400, 103552, 9494, 603, 1508, 2167, 774, 573, 11787, 9494, 235265, 108, 10142, 235269, 1452, 53088, 1486, 1593, 4282, 22786, 235269, 573, 3448, 577, 573, 2872, 603, 4247, 61690, 3890, 235292, 6287, 235269, 9494, 5999, 68589, 235265, 108, 886, 139, 1175, 11787, 9494, 9173, 492, 5999, 68589, 575, 573, 9494, 591, 235333, 7058, 995, 235251, 235269, 235248, 235284, 235276, 235276, 235315, 846, 653, 35005, 5999, 708, 6664, 575, 476, 87233, 576, 7257, 235269, 901, 476, 30361, 3287, 576, 122109, 9173, 492, 5999, 708, 573, 50602, 73201, 514, 47564, 674, 68589, 8761, 1497, 30257, 577, 1717, 1024, 4408, 222420, 10401, 948, 984, 20296, 5771, 577, 1736, 573, 116144, 970, 54416, 82582, 591, 37087, 1515, 578, 15037, 235269, 235248, 235284, 235276, 235276, 235284, 846, 108, 4157, 525, 8923, 13265, 5999, 68589, 1163, 1497, 30257, 575, 3590, 577, 11585, 591, 234363, 4669, 1008, 717, 1173, 235248, 235284, 235276, 235276, 235310, 235275, 578, 984, 68589, 774, 3724, 13265, 235290, 2896, 13102, 591, 235249, 235265, 235264, 1173, 166505, 578, 1334, 196442, 10277, 235275, 577, 1156, 12955, 591, 148250, 235269, 235248, 235284, 235276, 235276, 235304, 846, 108, 4738, 235290, 1202, 18113, 235269, 901, 2173, 235290, 21075, 147304, 44259, 791, 1125, 4699, 577, 68589, 575, 573, 11787, 9494, 575, 5001, 591, 23978, 1008, 717, 1173, 235248, 235284, 235276, 235274, 235284, 823, 578, 575, 58318, 578, 2173, 235290, 17877, 162683, 685, 1578, 591, 58010, 1530, 1008, 717, 1173, 235248, 235284, 235276, 235274, 235274, 846, 108, 2440, 43533, 235269, 9173, 492, 5999, 235269, 13265, 5999, 578, 44259, 1170, 68589, 2290, 103552, 3505, 235265, 7922, 49922, 235269, 2053, 235290, 1202, 18113, 44259, 56471, 577, 30635, 45197, 7257, 791, 577, 68589, 1163, 11873, 1497, 30257, 774, 573, 35544, 55005, 577, 1024, 4408, 13102, 591, 34432, 1605, 3638, 235269, 235248, 235284, 491, 1531, 235269, 169498, 8923, 30419, 846, 1]\n",
            "inputs:\n",
            "Human: Can brain cells move? By movement I mean long distance migration (preferably within the brain only).\n",
            "Assistant:The question is relatively broad and one should take into account that the brain not only consists of neurons, but also glial cells (supportive cells) and pre-mitotic neuronal stem cells. Furthermore, as critical fellow-scientists have indicated, developmental stage is very important, as the developing embryonic brain is very different from the adult brain.\n",
            "However, after sifting through various publications, the answer to the question is actually remarkably simple: Yes, brain cells migrate.\n",
            "In  the adult brain glial cells migrate in the brain (KlÃ¤mbt, 2009). Glial cells are involved in a myriad of functions, but a notable example of migrating glial cells are the oligodendrocytes that migrate relative long distances to find their target axons onto which they wrap themselves to form the insulating myelin sheath (Tsai and Miller, 2002).\n",
            "Neuronal stem cells migrate over long distances in response to injury (Imitola et al., 2004) and they migrate from specific stem-cell locations (e.g., hippocampus and subventricular zone) to other regions (Clarke, 2003).\n",
            "Post-mitotic, but non-differentiated neurons have been shown to migrate in the adult brain in fish (Scott et al., 2012), and in mammals and non-human primates as well (Sawada et al., 2011).\n",
            "Not surprisingly, glial cells, stem cells and neurons also migrate during embryonic development. Most notably, post-mitotic neurons destined to fulfill peripheral functions have to migrate over relatively long distances from the neural crest to their target locations (Neuroscience, 2nd ed, Neuronal Migration).<eos>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 651, 2872, 603, 11873, 7209, 578, 974, 1412, 1987, 1280, 3185, 674, 573, 9494, 780, 1297, 13569, 576, 44259, 235269, 901, 1170, 9173, 492, 5999, 591, 14186, 105065, 5999, 235275, 578, 953, 235290, 1202, 18113, 96057, 13265, 5999, 235265, 22174, 235269, 685, 9523, 11648, 235290, 129394, 3537, 791, 11969, 235269, 47492, 6325, 603, 1508, 2845, 235269, 685, 573, 11400, 103552, 9494, 603, 1508, 2167, 774, 573, 11787, 9494, 235265, 108, 10142, 235269, 1452, 53088, 1486, 1593, 4282, 22786, 235269, 573, 3448, 577, 573, 2872, 603, 4247, 61690, 3890, 235292, 6287, 235269, 9494, 5999, 68589, 235265, 108, 886, 139, 1175, 11787, 9494, 9173, 492, 5999, 68589, 575, 573, 9494, 591, 235333, 7058, 995, 235251, 235269, 235248, 235284, 235276, 235276, 235315, 846, 653, 35005, 5999, 708, 6664, 575, 476, 87233, 576, 7257, 235269, 901, 476, 30361, 3287, 576, 122109, 9173, 492, 5999, 708, 573, 50602, 73201, 514, 47564, 674, 68589, 8761, 1497, 30257, 577, 1717, 1024, 4408, 222420, 10401, 948, 984, 20296, 5771, 577, 1736, 573, 116144, 970, 54416, 82582, 591, 37087, 1515, 578, 15037, 235269, 235248, 235284, 235276, 235276, 235284, 846, 108, 4157, 525, 8923, 13265, 5999, 68589, 1163, 1497, 30257, 575, 3590, 577, 11585, 591, 234363, 4669, 1008, 717, 1173, 235248, 235284, 235276, 235276, 235310, 235275, 578, 984, 68589, 774, 3724, 13265, 235290, 2896, 13102, 591, 235249, 235265, 235264, 1173, 166505, 578, 1334, 196442, 10277, 235275, 577, 1156, 12955, 591, 148250, 235269, 235248, 235284, 235276, 235276, 235304, 846, 108, 4738, 235290, 1202, 18113, 235269, 901, 2173, 235290, 21075, 147304, 44259, 791, 1125, 4699, 577, 68589, 575, 573, 11787, 9494, 575, 5001, 591, 23978, 1008, 717, 1173, 235248, 235284, 235276, 235274, 235284, 823, 578, 575, 58318, 578, 2173, 235290, 17877, 162683, 685, 1578, 591, 58010, 1530, 1008, 717, 1173, 235248, 235284, 235276, 235274, 235274, 846, 108, 2440, 43533, 235269, 9173, 492, 5999, 235269, 13265, 5999, 578, 44259, 1170, 68589, 2290, 103552, 3505, 235265, 7922, 49922, 235269, 2053, 235290, 1202, 18113, 44259, 56471, 577, 30635, 45197, 7257, 791, 577, 68589, 1163, 11873, 1497, 30257, 774, 573, 35544, 55005, 577, 1024, 4408, 13102, 591, 34432, 1605, 3638, 235269, 235248, 235284, 491, 1531, 235269, 169498, 8923, 30419, 846, 1]\n",
            "labels:\n",
            "The question is relatively broad and one should take into account that the brain not only consists of neurons, but also glial cells (supportive cells) and pre-mitotic neuronal stem cells. Furthermore, as critical fellow-scientists have indicated, developmental stage is very important, as the developing embryonic brain is very different from the adult brain.\n",
            "However, after sifting through various publications, the answer to the question is actually remarkably simple: Yes, brain cells migrate.\n",
            "In  the adult brain glial cells migrate in the brain (KlÃ¤mbt, 2009). Glial cells are involved in a myriad of functions, but a notable example of migrating glial cells are the oligodendrocytes that migrate relative long distances to find their target axons onto which they wrap themselves to form the insulating myelin sheath (Tsai and Miller, 2002).\n",
            "Neuronal stem cells migrate over long distances in response to injury (Imitola et al., 2004) and they migrate from specific stem-cell locations (e.g., hippocampus and subventricular zone) to other regions (Clarke, 2003).\n",
            "Post-mitotic, but non-differentiated neurons have been shown to migrate in the adult brain in fish (Scott et al., 2012), and in mammals and non-human primates as well (Sawada et al., 2011).\n",
            "Not surprisingly, glial cells, stem cells and neurons also migrate during embryonic development. Most notably, post-mitotic neurons destined to fulfill peripheral functions have to migrate over relatively long distances from the neural crest to their target locations (Neuroscience, 2nd ed, Neuronal Migration).<eos>\n",
            "[INFO|configuration_utils.py:733] 2024-09-18 04:11:16,644 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-09-18 04:11:16,645 >> Model config GemmaConfig {\n",
            "  \"_name_or_path\": \"google/gemma-2b\",\n",
            "  \"architectures\": [\n",
            "    \"GemmaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"head_dim\": 256,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_activation\": null,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"gemma\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 18,\n",
            "  \"num_key_value_heads\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256000\n",
            "}\n",
            "\n",
            "model.safetensors.index.json: 100% 13.5k/13.5k [00:00<00:00, 51.5MB/s]\n",
            "[INFO|modeling_utils.py:3678] 2024-09-18 04:11:18,009 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.95G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 10.5M/4.95G [00:00<01:47, 45.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 41.9M/4.95G [00:00<00:34, 141MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 62.9M/4.95G [00:00<00:36, 133MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/4.95G [00:00<00:42, 115MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 105M/4.95G [00:00<00:36, 132MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 126M/4.95G [00:01<00:37, 129MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 147M/4.95G [00:01<00:47, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 168M/4.95G [00:01<00:43, 111MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 189M/4.95G [00:01<00:37, 126MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 210M/4.95G [00:01<00:34, 139MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 241M/4.95G [00:01<00:28, 163MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 273M/4.95G [00:01<00:25, 183MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 304M/4.95G [00:02<00:22, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 336M/4.95G [00:02<00:21, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 367M/4.95G [00:02<00:19, 234MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 398M/4.95G [00:02<00:18, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 430M/4.95G [00:02<00:18, 242MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 461M/4.95G [00:02<00:18, 244MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 493M/4.95G [00:02<00:18, 241MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 524M/4.95G [00:02<00:17, 253MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 556M/4.95G [00:03<00:16, 264MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 587M/4.95G [00:03<00:17, 245MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 619M/4.95G [00:03<00:18, 234MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 650M/4.95G [00:03<00:18, 237MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 682M/4.95G [00:03<00:18, 233MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 713M/4.95G [00:03<00:17, 244MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 744M/4.95G [00:03<00:17, 247MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 776M/4.95G [00:04<00:17, 241MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 807M/4.95G [00:04<00:22, 183MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 839M/4.95G [00:04<00:21, 190MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 860M/4.95G [00:04<00:22, 183MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 881M/4.95G [00:04<00:21, 188MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 902M/4.95G [00:04<00:21, 192MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 933M/4.95G [00:04<00:19, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 965M/4.95G [00:05<00:18, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 996M/4.95G [00:05<00:20, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.03G/4.95G [00:07<01:52, 34.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.06G/4.95G [00:07<01:20, 48.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.09G/4.95G [00:07<01:00, 64.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.11G/4.95G [00:08<00:51, 74.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.13G/4.95G [00:08<00:44, 85.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.15G/4.95G [00:08<00:37, 99.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.17G/4.95G [00:08<00:33, 112MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.20G/4.95G [00:08<00:30, 124MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.23G/4.95G [00:08<00:24, 150MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.26G/4.95G [00:08<00:21, 168MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.28G/4.95G [00:08<00:22, 162MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.30G/4.95G [00:09<00:26, 140MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.32G/4.95G [00:09<00:25, 143MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.34G/4.95G [00:09<00:25, 141MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.36G/4.95G [00:09<00:24, 149MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.38G/4.95G [00:09<00:22, 159MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.41G/4.95G [00:09<00:22, 159MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.43G/4.95G [00:09<00:21, 167MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.45G/4.95G [00:10<00:20, 174MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.47G/4.95G [00:11<01:04, 53.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.49G/4.95G [00:11<00:53, 65.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.51G/4.95G [00:11<00:49, 68.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.53G/4.95G [00:11<00:40, 83.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.55G/4.95G [00:11<00:40, 84.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.57G/4.95G [00:12<00:34, 99.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.59G/4.95G [00:12<00:30, 112MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.61G/4.95G [00:12<00:26, 127MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.65G/4.95G [00:12<00:22, 149MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.68G/4.95G [00:12<00:19, 170MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.71G/4.95G [00:12<00:18, 178MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.74G/4.95G [00:12<00:15, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.77G/4.95G [00:12<00:14, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.80G/4.95G [00:13<00:14, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.84G/4.95G [00:13<00:13, 234MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.87G/4.95G [00:13<00:13, 233MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.90G/4.95G [00:13<00:12, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.94G/4.95G [00:13<00:11, 270MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.97G/4.95G [00:13<00:11, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.00G/4.95G [00:13<00:13, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.03G/4.95G [00:14<00:12, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.07G/4.95G [00:14<00:14, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.10G/4.95G [00:14<00:13, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.13G/4.95G [00:14<00:12, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.16G/4.95G [00:14<00:11, 234MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.19G/4.95G [00:14<00:11, 247MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.22G/4.95G [00:14<00:10, 260MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.25G/4.95G [00:14<00:10, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.30G/4.95G [00:15<00:09, 272MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.34G/4.95G [00:15<00:08, 291MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.37G/4.95G [00:15<00:09, 263MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.40G/4.95G [00:15<00:09, 267MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.44G/4.95G [00:15<00:09, 269MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.47G/4.95G [00:15<00:09, 254MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.51G/4.95G [00:15<00:10, 236MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.54G/4.95G [00:16<00:10, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.57G/4.95G [00:16<00:10, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.61G/4.95G [00:16<00:09, 253MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.64G/4.95G [00:16<00:10, 227MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.68G/4.95G [00:16<00:08, 254MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.72G/4.95G [00:16<00:09, 243MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.75G/4.95G [00:16<00:08, 245MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.79G/4.95G [00:17<00:08, 264MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.82G/4.95G [00:17<00:08, 250MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.85G/4.95G [00:17<00:08, 255MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.89G/4.95G [00:17<00:07, 279MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.93G/4.95G [00:17<00:07, 268MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.97G/4.95G [00:17<00:06, 287MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.00G/4.95G [00:17<00:07, 275MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.03G/4.95G [00:18<00:07, 257MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.07G/4.95G [00:18<00:06, 269MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.10G/4.95G [00:18<00:07, 239MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.14G/4.95G [00:18<00:07, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.17G/4.95G [00:18<00:07, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.21G/4.95G [00:18<00:06, 263MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.24G/4.95G [00:18<00:06, 274MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.27G/4.95G [00:18<00:06, 266MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.30G/4.95G [00:19<00:06, 246MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.33G/4.95G [00:19<00:06, 258MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.37G/4.95G [00:19<00:06, 245MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.40G/4.95G [00:19<00:06, 257MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.44G/4.95G [00:19<00:05, 269MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.47G/4.95G [00:19<00:06, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.50G/4.95G [00:19<00:06, 238MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.53G/4.95G [00:20<00:06, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.57G/4.95G [00:20<00:06, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.60G/4.95G [00:20<00:06, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.63G/4.95G [00:20<00:06, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.66G/4.95G [00:20<00:05, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.70G/4.95G [00:20<00:05, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.73G/4.95G [00:20<00:04, 256MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.76G/4.95G [00:21<00:04, 255MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.80G/4.95G [00:21<00:04, 253MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.83G/4.95G [00:21<00:04, 260MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.86G/4.95G [00:21<00:04, 241MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.89G/4.95G [00:21<00:04, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.93G/4.95G [00:21<00:03, 268MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.96G/4.95G [00:21<00:03, 248MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.00G/4.95G [00:22<00:04, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.03G/4.95G [00:22<00:04, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.05G/4.95G [00:22<00:05, 156MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.07G/4.95G [00:22<00:06, 129MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.09G/4.95G [00:23<00:08, 105MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.11G/4.95G [00:23<00:07, 112MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.13G/4.95G [00:23<00:08, 100MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.15G/4.95G [00:23<00:07, 111MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.17G/4.95G [00:23<00:07, 107MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.19G/4.95G [00:24<00:08, 87.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.20G/4.95G [00:24<00:08, 88.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.22G/4.95G [00:24<00:09, 79.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.24G/4.95G [00:24<00:08, 88.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.27G/4.95G [00:24<00:05, 120MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.30G/4.95G [00:24<00:04, 140MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.32G/4.95G [00:25<00:04, 141MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.34G/4.95G [00:25<00:04, 141MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.36G/4.95G [00:25<00:03, 150MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.38G/4.95G [00:27<00:18, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.40G/4.95G [00:28<00:18, 28.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.45G/4.95G [00:28<00:10, 49.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.48G/4.95G [00:28<00:07, 66.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.50G/4.95G [00:28<00:05, 78.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.52G/4.95G [00:28<00:04, 92.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.55G/4.95G [00:28<00:03, 123MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.58G/4.95G [00:28<00:02, 146MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.61G/4.95G [00:29<00:01, 174MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.66G/4.95G [00:29<00:01, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.69G/4.95G [00:29<00:01, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.73G/4.95G [00:29<00:00, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.76G/4.95G [00:29<00:00, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.79G/4.95G [00:29<00:00, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.82G/4.95G [00:29<00:00, 246MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.85G/4.95G [00:29<00:00, 256MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.89G/4.95G [00:30<00:00, 248MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.92G/4.95G [00:30<00:00, 250MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.95G/4.95G [00:30<00:00, 163MB/s]\n",
            "Downloading shards:  50% 1/2 [00:30<00:30, 30.84s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/67.1M [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 31.5M/67.1M [00:00<00:00, 202MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 67.1M/67.1M [00:00<00:00, 227MB/s]\n",
            "Downloading shards: 100% 2/2 [00:31<00:00, 15.82s/it]\n",
            "[INFO|modeling_utils.py:1606] 2024-09-18 04:11:49,645 >> Instantiating GemmaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1038] 2024-09-18 04:11:49,647 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:328] 2024-09-18 04:11:49,778 >> `config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
            "Loading checkpoint shards: 100% 2/2 [00:23<00:00, 11.93s/it]\n",
            "[INFO|modeling_utils.py:4507] 2024-09-18 04:12:14,051 >> All model checkpoint weights were used when initializing GemmaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4515] 2024-09-18 04:12:14,051 >> All the weights of GemmaForCausalLM were initialized from the model checkpoint at google/gemma-2b.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GemmaForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 137/137 [00:00<00:00, 836kB/s]\n",
            "[INFO|configuration_utils.py:993] 2024-09-18 04:12:14,568 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b/snapshots/68e273d91b1d6ea57c9e6024c4f887832f7b43fa/generation_config.json\n",
            "[INFO|configuration_utils.py:1038] 2024-09-18 04:12:14,568 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "09/18/2024 04:12:14 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "09/18/2024 04:12:14 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "09/18/2024 04:12:14 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "09/18/2024 04:12:14 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "09/18/2024 04:12:14 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,up_proj,down_proj,v_proj,gate_proj,k_proj,o_proj\n",
            "09/18/2024 04:12:15 - INFO - llamafactory.model.loader - trainable params: 9,805,824 || all params: 2,515,978,240 || trainable%: 0.3897\n",
            "[INFO|trainer.py:648] 2024-09-18 04:12:15,548 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2134] 2024-09-18 04:12:16,051 >> ***** Running training *****\n",
            "[INFO|trainer.py:2135] 2024-09-18 04:12:16,052 >>   Num examples = 1,029\n",
            "[INFO|trainer.py:2136] 2024-09-18 04:12:16,052 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2137] 2024-09-18 04:12:16,052 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2140] 2024-09-18 04:12:16,052 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2141] 2024-09-18 04:12:16,052 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2142] 2024-09-18 04:12:16,052 >>   Total optimization steps = 192\n",
            "[INFO|trainer.py:2143] 2024-09-18 04:12:16,056 >>   Number of trainable parameters = 9,805,824\n",
            "  3% 5/192 [06:33<3:51:16, 74.21s/it]09/18/2024 04:18:49 - INFO - llamafactory.train.callbacks - {'loss': 3.0157, 'learning_rate': 4.9916e-05, 'epoch': 0.08, 'throughput': 151.13}\n",
            "{'loss': 3.0157, 'grad_norm': 3.567939281463623, 'learning_rate': 4.991638098272951e-05, 'epoch': 0.08, 'num_input_tokens_seen': 59488}\n",
            "  5% 10/192 [12:39<3:43:51, 73.80s/it]09/18/2024 04:24:55 - INFO - llamafactory.train.callbacks - {'loss': 2.6661, 'learning_rate': 4.9666e-05, 'epoch': 0.16, 'throughput': 151.6}\n",
            "{'loss': 2.6661, 'grad_norm': 6.345503807067871, 'learning_rate': 4.966608330212198e-05, 'epoch': 0.16, 'num_input_tokens_seen': 115168}\n",
            "  8% 15/192 [18:45<3:36:26, 73.37s/it]09/18/2024 04:31:01 - INFO - llamafactory.train.callbacks - {'loss': 2.3622, 'learning_rate': 4.9251e-05, 'epoch': 0.23, 'throughput': 151.65}\n",
            "{'loss': 2.3622, 'grad_norm': 2.1966192722320557, 'learning_rate': 4.9250781329863606e-05, 'epoch': 0.23, 'num_input_tokens_seen': 170720}\n",
            " 10% 20/192 [24:42<3:20:09, 69.82s/it]09/18/2024 04:36:58 - INFO - llamafactory.train.callbacks - {'loss': 2.2229, 'learning_rate': 4.8673e-05, 'epoch': 0.31, 'throughput': 151.77}\n",
            "{'loss': 2.2229, 'grad_norm': 4.07383918762207, 'learning_rate': 4.867325323737765e-05, 'epoch': 0.31, 'num_input_tokens_seen': 224928}\n",
            " 13% 25/192 [31:03<3:28:43, 74.99s/it]09/18/2024 04:43:19 - INFO - llamafactory.train.callbacks - {'loss': 2.2204, 'learning_rate': 4.7937e-05, 'epoch': 0.39, 'throughput': 151.87}\n",
            "{'loss': 2.2204, 'grad_norm': 6.835359573364258, 'learning_rate': 4.793736241118728e-05, 'epoch': 0.39, 'num_input_tokens_seen': 282992}\n",
            " 16% 30/192 [37:06<3:11:41, 70.99s/it]09/18/2024 04:49:22 - INFO - llamafactory.train.callbacks - {'loss': 2.3234, 'learning_rate': 4.7048e-05, 'epoch': 0.47, 'throughput': 151.9}\n",
            "{'loss': 2.3234, 'grad_norm': 0.5589499473571777, 'learning_rate': 4.7048031608708876e-05, 'epoch': 0.47, 'num_input_tokens_seen': 338256}\n",
            " 18% 35/192 [43:22<3:23:19, 77.70s/it]09/18/2024 04:55:38 - INFO - llamafactory.train.callbacks - {'loss': 2.2916, 'learning_rate': 4.6011e-05, 'epoch': 0.54, 'throughput': 151.93}\n",
            "{'loss': 2.2916, 'grad_norm': 5.244668483734131, 'learning_rate': 4.601121002736095e-05, 'epoch': 0.54, 'num_input_tokens_seen': 395408}\n",
            " 21% 40/192 [49:27<3:13:15, 76.29s/it]09/18/2024 05:01:43 - INFO - llamafactory.train.callbacks - {'loss': 2.1438, 'learning_rate': 4.4834e-05, 'epoch': 0.62, 'throughput': 151.91}\n",
            "{'loss': 2.1438, 'grad_norm': 0.7283533215522766, 'learning_rate': 4.4833833507280884e-05, 'epoch': 0.62, 'num_input_tokens_seen': 450832}\n",
            " 23% 45/192 [55:16<2:44:58, 67.34s/it]09/18/2024 05:07:32 - INFO - llamafactory.train.callbacks - {'loss': 2.0210, 'learning_rate': 4.3524e-05, 'epoch': 0.70, 'throughput': 151.9}\n",
            "{'loss': 2.021, 'grad_norm': 0.9470729231834412, 'learning_rate': 4.352377813387398e-05, 'epoch': 0.7, 'num_input_tokens_seen': 503824}\n",
            " 26% 50/192 [1:01:10<2:43:20, 69.01s/it]09/18/2024 05:13:26 - INFO - llamafactory.train.callbacks - {'loss': 2.1369, 'learning_rate': 4.2090e-05, 'epoch': 0.78, 'throughput': 151.87}\n",
            "{'loss': 2.1369, 'grad_norm': 0.5262941718101501, 'learning_rate': 4.208980755057178e-05, 'epoch': 0.78, 'num_input_tokens_seen': 557424}\n",
            " 29% 55/192 [1:07:21<2:38:46, 69.54s/it]09/18/2024 05:19:37 - INFO - llamafactory.train.callbacks - {'loss': 2.1354, 'learning_rate': 4.0542e-05, 'epoch': 0.85, 'throughput': 151.92}\n",
            "{'loss': 2.1354, 'grad_norm': 0.5100542306900024, 'learning_rate': 4.054151433425194e-05, 'epoch': 0.85, 'num_input_tokens_seen': 613920}\n",
            " 31% 60/192 [1:13:25<2:42:30, 73.87s/it]09/18/2024 05:25:41 - INFO - llamafactory.train.callbacks - {'loss': 2.1736, 'learning_rate': 3.8889e-05, 'epoch': 0.93, 'throughput': 151.94}\n",
            "{'loss': 2.1736, 'grad_norm': 0.3503081798553467, 'learning_rate': 3.888925582549006e-05, 'epoch': 0.93, 'num_input_tokens_seen': 669344}\n",
            " 34% 65/192 [1:19:31<2:29:37, 70.69s/it]09/18/2024 05:31:47 - INFO - llamafactory.train.callbacks - {'loss': 2.2056, 'learning_rate': 3.7144e-05, 'epoch': 1.01, 'throughput': 151.95}\n",
            "{'loss': 2.2056, 'grad_norm': 0.535907506942749, 'learning_rate': 3.7144084842908505e-05, 'epoch': 1.01, 'num_input_tokens_seen': 725064}\n",
            " 36% 70/192 [1:25:21<2:16:15, 67.01s/it]09/18/2024 05:37:37 - INFO - llamafactory.train.callbacks - {'loss': 2.1555, 'learning_rate': 3.5318e-05, 'epoch': 1.09, 'throughput': 151.95}\n",
            "{'loss': 2.1555, 'grad_norm': 0.5363945364952087, 'learning_rate': 3.5317675745109866e-05, 'epoch': 1.09, 'num_input_tokens_seen': 778136}\n",
            " 36% 70/192 [1:25:21<2:16:15, 67.01s/it]"
          ]
        }
      ],
      "source": [
        "%cd /content/LLaMA-Factory/\n",
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgR3UFhB0Ifq"
      },
      "source": [
        "## Fine-tune model via Command Line\n",
        "\n",
        "It takes ~30min for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS0Qk5OR0i4Q"
      },
      "outputs": [],
      "source": [
        "# Fine-tune model via command line\n",
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                        # do supervised fine-tuning\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"neuralmagic/gemma-2-2b-it-quantized.w8a16\", # use Gemma 2B quantized model\n",
        "  dataset=\"identity,alpaca_en_demo\",  # use alpaca and identity datasets\n",
        "  template=\"gemma2b\",                 # appropriate prompt template\n",
        "  finetuning_type=\"lora\",             # use LoRA adapters to save memory\n",
        "  lora_target=\"all\",                  # attach LoRA adapters to all linear layers\n",
        "  output_dir=\"gemma2b_lora\",          # path to save LoRA adapters\n",
        "  per_device_train_batch_size=1,      # reduced batch size\n",
        "  gradient_accumulation_steps=2,      # reduced gradient accumulation steps\n",
        "  lr_scheduler_type=\"cosine\",         # use cosine learning rate scheduler\n",
        "  logging_steps=10,                   # log every 10 steps\n",
        "  warmup_ratio=0.1,                   # use warmup scheduler\n",
        "  save_steps=500,                     # save checkpoint every 500 steps\n",
        "  learning_rate=3e-5,                 # lower learning rate\n",
        "  num_train_epochs=1.0,               # reduce number of epochs\n",
        "  max_samples=100,                    # use fewer examples to fit within memory\n",
        "  max_grad_norm=1.0,                  # clip gradient norm to 1.0\n",
        "  loraplus_lr_ratio=8.0,              # lower lambda to reduce computation\n",
        "  fp16=True,                          # use float16 mixed precision training\n",
        "  use_liger_kernel=True,              # use liger kernel for efficient training\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_gemma2b.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "!llamafactory-cli train train_gemma2b.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVNaC-xS5N40"
      },
      "source": [
        "## Infer the fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh8H9A_25SF9"
      },
      "outputs": [],
      "source": [
        "# Infer the fine-tuned model\n",
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"neuralmagic/gemma-2-2b-it-quantized.w8a16\", # use Gemma 2B quantized model\n",
        "  adapter_name_or_path=\"gemma2b_lora\",   # load the saved LoRA adapters\n",
        "  template=\"gemma2b\",                    # same as in training\n",
        "  finetuning_type=\"lora\",                # same as in training\n",
        "  quantization_bit=4,                    # use 4-bit quantized model\n",
        ")\n",
        "\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "messages = []\n",
        "print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    torch_gc()\n",
        "    print(\"History has been removed.\")\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "torch_gc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTESHaFvbNTr"
      },
      "source": [
        "## Merge the LoRA adapter and optionally upload model\n",
        "\n",
        "NOTE: the Colab free version has merely 12GB RAM, where merging LoRA of a 8B model needs at least 18GB RAM, thus you **cannot** perform it in the free version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcNcHcA4bf4Z"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMojogHbaOZF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Update the model and adapter paths for Gemma 2B\n",
        "args = dict(\n",
        "  model_name_or_path=\"neuralmagic/gemma-2-2b-it-quantized.w8a16\",  # use Gemma 2B quantized model\n",
        "  adapter_name_or_path=\"gemma2b_lora\",  # load the saved LoRA adapters from the earlier training\n",
        "  template=\"gemma2b\",  # use the same template as in training\n",
        "  finetuning_type=\"lora\",  # same as in training\n",
        "  export_dir=\"gemma2b_lora_merged\",  # the path to save the merged model\n",
        "  export_size=2,  # the file shard size (in GB) of the merged model\n",
        "  export_device=\"cpu\",  # choose 'cpu' or 'cuda' based on available resources\n",
        "  #export_hub_model_id=\"your_id/your_model\",  # your Hugging Face hub ID to upload model, uncomment if needed\n",
        ")\n",
        "\n",
        "# Save the updated configuration to a JSON file\n",
        "json.dump(args, open(\"merge_gemma2b.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "# Export the model using the updated configuration\n",
        "!llamafactory-cli export merge_gemma2b.json\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "89e6d6ef166d49c3a4b017d041b9833b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93184fb2e5d144f6b1891f7c03bb9a30",
              "IPY_MODEL_3ae4731ab2fe43e9aed51352b7706551",
              "IPY_MODEL_0290a2e40e7840e9ac8d8889fd386f73"
            ],
            "layout": "IPY_MODEL_89f4f7137e8242ba85c59eff29ea2900"
          }
        },
        "e6554619029345a3ac25b092ce68283f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a6942125b024a8ea137f14e204e876b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_802f7639a1ec4f7d8d19930ae0d50559",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "cef78e80bbe4449083ceefcd03853423": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_53cd9ae24080479a85a6135ee345259f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_20605af761d94a97b102ef44138b0028",
            "value": ""
          }
        },
        "4af063573c1f4dad945ef0954b1280e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_19ba8358feed4049b49a4a530605c52b",
            "style": "IPY_MODEL_fcf5109c259441ceb0a92e15236ea3d2",
            "value": false
          }
        },
        "eaa684eeb1914f26aacbf5599200c75d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_b86b008d321345b2b8bdaecd9c4e2fc4",
            "style": "IPY_MODEL_c1d49c792fd045b3b4e8747a8753b62a",
            "tooltip": ""
          }
        },
        "c12069af9f454083bdacc874a62534fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1473b696888a444983d745882f59acd6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3c1c6fcc397a4d4dad0c699832ca1b22",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "89f4f7137e8242ba85c59eff29ea2900": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "9a6942125b024a8ea137f14e204e876b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "802f7639a1ec4f7d8d19930ae0d50559": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53cd9ae24080479a85a6135ee345259f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20605af761d94a97b102ef44138b0028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19ba8358feed4049b49a4a530605c52b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcf5109c259441ceb0a92e15236ea3d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b86b008d321345b2b8bdaecd9c4e2fc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1d49c792fd045b3b4e8747a8753b62a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "1473b696888a444983d745882f59acd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c1c6fcc397a4d4dad0c699832ca1b22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0048e5323d74f08afe8b58c976f98ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_540b108657934a6ea42932e0a424c870",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0e6d417614b545229b93dee08e157f82",
            "value": "Connecting..."
          }
        },
        "540b108657934a6ea42932e0a424c870": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e6d417614b545229b93dee08e157f82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93184fb2e5d144f6b1891f7c03bb9a30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a33d68d9a3f941448c96feb485b35833",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_67e8a2f9635a4011ac295a2b6220b8b4",
            "value": "Token is valid (permission: fineGrained)."
          }
        },
        "3ae4731ab2fe43e9aed51352b7706551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dbd52d941c040509dfb90b87aaf1825",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_339a85f8e7dd47b4b61dc1bdde400817",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "0290a2e40e7840e9ac8d8889fd386f73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81833313b36c4ea4b8545059f1b85d00",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c6b8d0eddfb44058a763c5893675bff7",
            "value": "Login successful"
          }
        },
        "a33d68d9a3f941448c96feb485b35833": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67e8a2f9635a4011ac295a2b6220b8b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dbd52d941c040509dfb90b87aaf1825": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "339a85f8e7dd47b4b61dc1bdde400817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81833313b36c4ea4b8545059f1b85d00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6b8d0eddfb44058a763c5893675bff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}