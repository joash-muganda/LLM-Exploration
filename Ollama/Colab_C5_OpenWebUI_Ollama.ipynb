{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **OpenWebUI_Ollama**"
      ],
      "metadata": {
        "id": "nF3yg3AHk1Re"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Overview**\n",
        "This notebook outlines the process I followed to set up Open WebUI with Ollama on my local machine, including using Docker for container management and interacting with the command line and web interface. This setup involved configuring Ollama as an API, managing Docker containers, and downloading a model both via the command line and the web interface to start using it in the WebUI.\n",
        "\n",
        "**Step 1: Initial Setup**\n",
        "**1.1 Install Docker **\n",
        "I ensured that Docker was installed on my machine for running Open WebUI in a containerized environment. Docker was essential for creating and managing the Open WebUI environment and integrating it with the Ollama LLM models.\n",
        "\n",
        "## **Command used:**\n",
        "docker --version\n",
        "Step 2: Running the Open WebUI with Ollama\n",
        "2.1 Pulling Open WebUI Docker Image\n",
        "To get started with Open WebUI, I pulled the Open WebUI Docker image that comes pre-bundled with Ollama:\n",
        "\n",
        "\n",
        "**docker pull ghcr.io/open-webui/open-webui:ollama**\n",
        "2.2 Starting Open WebUI\n",
        "After pulling the Docker image, I ran the container using the following command to expose the WebUI at localhost:3000:\n",
        "\n",
        "docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\n",
        "This command mapped the necessary volumes for data persistence and specified that the container should restart automatically if it crashes.\n",
        "\n",
        "Step 3: Working with Ollama\n",
        "3.1 Checking Ollama Status\n",
        "Once the Open WebUI container was running, I checked that Ollama was also up and running as an API at localhost:11434:\n",
        "\n",
        "\n",
        "\n",
        "curl http://localhost:11434\n",
        "The response confirmed that \"Ollama is running.\"\n",
        "\n",
        "3.2 Pulling the Gemma2:2B Model via Command Line\n",
        "Before downloading the model in the WebUI, I used the Ollama command line interface to pull the Gemma2:2B model:\n",
        "\n",
        "\n",
        "**ollama pull gemma2:2b**\n",
        "This command downloaded the model files, but I needed to enable them in Open WebUI.\n",
        "\n",
        "Step 4: Accessing Open WebUI and Downloading Models\n",
        "4.1 Accessing Open WebUI\n",
        "With Docker and Ollama running, I accessed the Open WebUI interface at localhost:3000 using my browser. The Open WebUI interface allows users to manage models, pull new ones, and run various LLM tasks.\n",
        "\n",
        "4.2 Pulling the Model via WebUI\n",
        "Even though the Gemma2:2B model was already pulled via the command line, I needed to make it available in the WebUI for use. To do this, I navigated to the Models section in Open WebUI, entered the model tag (gemma2:2b), and pulled it again directly through the WebUI.\n",
        "\n",
        "Step 5: Confirmation and Usage\n",
        "After successfully pulling the model in Open WebUI, it became available for use. I could now select the Gemma2:2B model from the dropdown menu in the WebUI to start running queries and generating responses.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "By following the steps above, I was able to successfully set up Open WebUI with Ollama, pull a large language model (Gemma2:2B), and begin using it for tasks directly through the Open WebUI. This combination of Docker, command-line tools, and a web interface provided a flexible and powerful environment for experimenting with models.\n",
        "\n"
      ],
      "metadata": {
        "id": "8ybltIJij6CV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GrtuQCs5lGJ1"
      }
    }
  ]
}